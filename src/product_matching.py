# -*- coding: utf-8 -*-
"""Product Matching.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UrS-_MuxWtvjQGgVCIMydR_iF51SOgli

# READ THE CSV MAKE DF
"""

import pandas as pd

df =pd.read_csv("/content/flipkart_com-ecommerce_sample.csv")
df.describe(include='all').T

"""# Vector DB

## EMBEDD THE STRING AND IMAGE. MAKE A DICT TO MAINTAIN THE ID AND INDEX MATCHING
"""

from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel

# Load CLIP model and processor
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Inputs: Text and Image
text1 = "Key Features of Alisha Solid Women's Cycling Shorts: Pack of 3, made of Cotton Lycra fabric, ideal for women. Designed as cycling shorts with a solid pattern. Gentle machine wash in lukewarm water; avoid bleach. Style Code: ALTHT_3P_21. Includes 3 shorts in the sales package."
text2= "Key Features of Alisha Solid Women's Cycling Shorts Cotton Lycra Navy, Red, Navy,Specifications of Alisha Solid Women's Cycling Shorts Shorts Details Number of Contents in Sales Package Pack of 3"
image_path = "/content/image.jpeg"  # Replace with your image path
image = Image.open(image_path)

image_path2 = "/content/image2.jpeg"  # Replace with your image path
image2 = Image.open(image_path2)
# Preprocess inputs
inputs = processor(text=[text1,text2], images=[image,image2], return_tensors="pt", padding=True)

# Generate embeddings
outputs = model(**inputs)
text_embeddings = outputs.text_embeds  # Text embedding
image_embeddings = outputs.image_embeds  # Image embedding

# Normalize embeddings (optional, for similarity search)
text_embeddings = text_embeddings / text_embeddings.norm(p=2, dim=-1, keepdim=True)
image_embeddings = image_embeddings / image_embeddings.norm(p=2, dim=-1, keepdim=True)


print("Text Embedding Shape:", text_embeddings.shape)
print("Image Embedding Shape:", image_embeddings.shape)

import requests
from io import BytesIO
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import math

count=0
SAMPLES=20

index_map={}
indx = 0
length=70
embedding =torch.empty(0)
for rows in df.iterrows():
  if count<SAMPLES:
    if count%10==0:
        print(count)
    count+=1
    images=[]
    for url in rows[1]['image'][1:-1].split(','):
        if url:
            try:
                # print(url)
                response = requests.get(url.replace('"',""), stream=True)
                response.raise_for_status()
                image = Image.open(BytesIO(response.content))
                image_path=url.split('/')[-1].split(".")[0]+".jpg"
                image.save(image_path)
                image = Image.open(image_path)
                images.append(image)
            except:
                continue
    if len(rows[1]['description'][0:70])>0 and len(images)>0:
        text = rows[1]['description']
        inputs = processor(text=[text[i:i + length] for i in range(0, len(text), length)], images=images, return_tensors="pt", padding=True)
        # Generate embeddings
        outputs = model(**inputs)
        text_embeddings = outputs.text_embeds  # Text embedding
        image_embeddings = outputs.image_embeds  # Image embedding
        index_map[rows[1]['uniq_id']]=[indx,indx+len(images)+math.ceil(len(text)/length)+1]
        indx += len(images)+math.ceil(len(text)/length)+1
        embedding = torch.cat((embedding,text_embeddings),0)
        embedding = torch.cat((embedding,image_embeddings),0)
print("EMBEDD SIZE:", embedding.shape)

"""# QUERY"""

from sentence_transformers import SentenceTransformer
import torch
def search(query, embedding, index_map, dataframe,k=5 ):
    model = SentenceTransformer("clip-ViT-B-32")
    query_emb = model.encode(query)

    similarity_scores = []
    for key, value in index_map.items():
      score = model.similarity(embedding[value[0]:value[1]],query_emb).max()
      similarity_scores.append((key,score))

    similarity_scores.sort(key=lambda x: x[1], reverse=True)
    for uniq_id in similarity_scores[:k]:
        link =dataframe[dataframe['uniq_id']==uniq_id[0]]['image']
        print(f"Uniq ID: {uniq_id[0]}, Similarity Score: {uniq_id[1]} ")
        for l in link:
            print(l)
        print("#"*100)

search("leather black",embedding=embedding,index_map=index_map, dataframe =df)

from sentence_transformers import SentenceTransformer
import torch
def search_image(image_path, embedding, index_map, dataframe,k=5 ):
    image = Image.open(image_path)
    model = SentenceTransformer("clip-ViT-B-32")
    query_emb = model.encode(image)

    similarity_scores = []
    for key, value in index_map.items():
      score = model.similarity(embedding[value[0]:value[1]],query_emb).max()
      similarity_scores.append((key,score))

    similarity_scores.sort(key=lambda x: x[1], reverse=True)
    for uniq_id in similarity_scores[:k]:
        link =dataframe[dataframe['uniq_id']==uniq_id[0]]['image']
        print(f"Uniq ID: {uniq_id[0]}, Similarity Score: {uniq_id[1]} ")
        for l in link:
            print(l)
        print("#"*100)

search_image("/content/test.jpg",embedding=embedding,index_map=index_map, dataframe =df,k=2)

search_image("/content/test2.jpg",embedding=embedding,index_map=index_map, dataframe =df,k=2)

"""# Save pickle files"""

import pickle

def pickle_file(file_name,data):
    with open(file_name, 'wb') as f:
        pickle.dump(data, f)
def read_pickle_file(file_name):
    with open(file_name, 'rb') as f:
        return pickle.load(f)

pickle_file('embeddings.pickle',embedding)
pickle_file('index_map.pickle',index_map)
pickle_file('dataframe.pickle',df)

import pickle
embeddings_read = read_pickle_file('embeddings.pickle')
index_map_read = read_pickle_file('index_map.pickle')
df_read = read_pickle_file('dataframe.pickle')

embeddings_read.shape

len(index_map_read)

df_read.shape

search("leather black",embedding=embeddings_read,index_map=index_map_read, dataframe=df_read)

search("fabric",embedding=embeddings_read,index_map=index_map_read, dataframe=df_read)

